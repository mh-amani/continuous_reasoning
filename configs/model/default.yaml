_target_: src.models.base.LitModuleBase

model_params:
  compile: false # compile model for faster training with pytorch 2.0
  # Enable/disable saving samples from forward pass (saves to JSONL files)
  save_samples_enabled: true
  # Number of samples to save per epoch per stage (learn/val/test) - taken from first batch
  num_samples_to_save: 10
  save_samples_to_wandb: true

optimizer:
  _target_: torch.optim.AdamW
  # _partial_: true # it will return a partial function
  lr: 0.005
  weight_decay: 0.001

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  # _partial_: true # it will return a partial function
  mode: min
  factor: 0.95
  patience: 5
  threshold: 0.1
  threshold_mode: "rel" # rel or abs
  cooldown: 2
  min_lr: 1e-7
  eps: 1e-8

  scheduler_config:
    interval: "epoch" # The unit of the scheduler's step size. 'step' or 'epoch
    frequency: 1 # corresponds to updating the learning rate after every `frequency` epoch/step
    monitor: ${model.model_params.monitor}
    strict: False

